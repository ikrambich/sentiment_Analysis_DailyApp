# -*- coding: utf-8 -*-
"""Copy of text sentiment analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u4mSj1eqFZCVRk1Tb1IU8Sh83cvevQX3
"""

import pandas as pd
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split

# Load your dataset
df = pd.read_csv("training.csv")
texts = df['text'].astype(str).values
labels = df['label'].values
num_classes = len(set(labels))


# Tokenization
tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
padded = pad_sequences(sequences, maxlen=50, padding='post')



# Convert labels to one-hot
labels_cat = to_categorical(labels, num_classes=num_classes)

# Split dataset
X_train, X_val, y_train, y_val = train_test_split(padded, labels_cat, test_size=0.2, random_state=42)

# Build the model
model = Sequential([
    Embedding(input_dim=10000, output_dim=64, input_length=50),
    Bidirectional(LSTM(64)),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

# Compile
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train
early_stop = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)
model.fit(X_train, y_train,
          epochs=50, batch_size=32,
          validation_data=(X_val, y_val),
          callbacks=[early_stop])

# Evaluate
val_loss, val_acc = model.evaluate(X_val, y_val)
print(f"Validation Accuracy: {val_acc:.2f}")

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Predict class probabilities
y_val_probs = model.predict(X_val)

# Convert probabilities to predicted class indices
y_val_pred = np.argmax(y_val_probs, axis=1)
y_val_true = np.argmax(y_val, axis=1)

# Accuracy
accuracy = accuracy_score(y_val_true, y_val_pred)
print(f"Validation Accuracy: {accuracy:.4f}")

# Classification Report
print("\nClassification Report:")
print(classification_report(y_val_true, y_val_pred))

# Confusion Matrix
conf_matrix = confusion_matrix(y_val_true, y_val_pred)

plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

# Optional: Error Analysis (Example Tweets)
# Assuming you still have access to the raw text data
from sklearn.model_selection import train_test_split

texts_train, texts_val = train_test_split(texts, test_size=0.2, random_state=42)

print("\nExamples of Misclassified Tweets:")
misclassified_indices = np.where(y_val_true != y_val_pred)[0]
for i in misclassified_indices[:5]:  # Show first 5 errors
    print(f"Tweet: {texts_val[i]}")
    print(f"True Label: {y_val_true[i]}, Predicted: {y_val_pred[i]}\n")

!pip install keras-tuner

import keras_tuner as kt
from tensorflow.keras.optimizers import Adam

# Load training, validation, and test datasets
train_df = pd.read_csv("training.csv")
test_df = pd.read_csv("test.csv")

# Extract texts and labels
texts_train = train_df['text'].astype(str).values
labels_train = train_df['label'].values



texts_test = test_df['text'].astype(str).values
labels_test = test_df['label'].values

tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')
tokenizer.fit_on_texts(texts_train)

# Convert texts to sequences
train_seq = tokenizer.texts_to_sequences(texts_train)
val_seq = tokenizer.texts_to_sequences(texts_val)
test_seq = tokenizer.texts_to_sequences(texts_test)

# Pad all
maxlen = 50
X_train = pad_sequences(train_seq, maxlen=maxlen, padding='post')
X_val = pad_sequences(val_seq, maxlen=maxlen, padding='post')
X_test = pad_sequences(test_seq, maxlen=maxlen, padding='post')

from tensorflow.keras.utils import to_categorical

num_classes = len(set(labels_train)  | set(labels_test))

y_train = to_categorical(labels_train, num_classes)
y_test = to_categorical(labels_test, num_classes)

def build_model(hp):
    model = Sequential()
    model.add(Embedding(input_dim=10000,
                        output_dim=hp.Choice('embedding_dim', [32, 64, 128]),
                        input_length=50))
    model.add(Bidirectional(LSTM(units=hp.Int('lstm_units', min_value=32, max_value=128, step=32))))
    model.add(Dropout(rate=hp.Float('dropout1', 0.2, 0.5, step=0.1)))
    model.add(Dense(units=hp.Int('dense_units', 32, 128, step=32), activation='relu'))
    model.add(Dropout(rate=hp.Float('dropout2', 0.2, 0.5, step=0.1)))
    model.add(Dense(num_classes, activation='softmax'))

    model.compile(
        optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

# Initialize tuner
tuner = kt.RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=3,
    executions_per_trial=1,
    directory='tuner_dir',
    project_name='lstm_tuning'
)

# Early stopping
early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Search for best hyperparameters
tuner.search(X_train, y_train,
             epochs=10,
             validation_data=(X_val, y_val),
             callbacks=[early_stop],
             batch_size=32)
# Retrieve best model
best_model = tuner.get_best_models(num_models=1)[0]
# Evaluate
val_loss, val_acc = best_model.evaluate(X_val, y_val)
print(f"Tuned Validation Accuracy: {val_acc:.4f}")

# Optional: See best hyperparameters
best_hps = tuner.get_best_hyperparameters(1)[0]
print("Best Hyperparameters:")
for hp in ['embedding_dim', 'lstm_units', 'dense_units', 'dropout1', 'dropout2', 'learning_rate']:
    print(f"{hp}: {best_hps.get(hp)}")

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Predict class probabilities
y_val_probs = best_model.predict(X_val)

# Convert probabilities to predicted class indices
y_val_pred = np.argmax(y_val_probs, axis=1)
y_val_true = np.argmax(y_val, axis=1)

# Accuracy
accuracy = accuracy_score(y_val_true, y_val_pred)
print(f"Validation Accuracy: {accuracy:.4f}")

# Classification Report
print("\nClassification Report:")
print(classification_report(y_val_true, y_val_pred))

# Confusion Matrix
conf_matrix = confusion_matrix(y_val_true, y_val_pred)

plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

# Optional: Error Analysis (Example Tweets)
# Assuming you still have access to the raw text data
from sklearn.model_selection import train_test_split

texts_train, texts_val = train_test_split(texts, test_size=0.2, random_state=42)

print("\nExamples of Misclassified Tweets:")
misclassified_indices = np.where(y_val_true != y_val_pred)[0]
for i in misclassified_indices[:5]:  # Show first 5 errors
    print(f"Tweet: {texts_val[i]}")
    print(f"True Label: {y_val_true[i]}, Predicted: {y_val_pred[i]}\n")

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score

# Step 1: Load test data
test_df = pd.read_csv('test.csv')
test_texts = test_df['text'].astype(str).values
test_labels = test_df['label'].values  # assuming it has 'label'

# Step 2: Tokenize and pad
test_sequences = tokenizer.texts_to_sequences(test_texts)
test_padded = pad_sequences(test_sequences, maxlen=50, padding='post')

# Step 3: Predict
pred_prob = model.predict(test_padded)
pred_labels = np.argmax(pred_prob, axis=1)

# Step 4: Compute accuracy
test_accuracy = accuracy_score(test_labels, pred_labels)
print(f"Test Accuracy: {test_accuracy:.2f}")

# Example sentence(s)

new_texts = ["i feel so happy today", "i am extremely sad and tired","i feel sad now","i will never ask about their opinion again","i feel pain "]

# Tokenize and pad
new_seq = tokenizer.texts_to_sequences(new_texts)
new_pad = pad_sequences(new_seq, maxlen=50, padding='post')

# Predict
pred_probs = model.predict(new_pad)
pred_classes = np.argmax(pred_probs, axis=1)

# Print results
for text, label in zip(new_texts, pred_classes):
    print(f"'{text}' â†’ Predicted Label: {label}")

model.save('sentiment_model.h5')